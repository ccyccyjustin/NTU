{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f923351-0f3a-4c6a-907c-308c88050dd5",
   "metadata": {},
   "source": [
    "# Tutorial 3 at NTU: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216d21b-72d0-4bea-acc9-bce9f0dee4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec45138-f039-4388-89cb-3b6588451882",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403ad83-7cab-4753-9ffc-cee49a4c546b",
   "metadata": {},
   "source": [
    "Principal Component Analysis is a traditional multivariate statistical technique for dimension or variable reduction. \n",
    "\n",
    "It aims to find linear combinations of original variables such that the information in the original data is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ba64a-c06b-4bcd-8d43-8434ec229900",
   "metadata": {},
   "source": [
    "### 1.1 Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8c9c6-c9dc-4267-8b68-f38be4fddd1c",
   "metadata": {},
   "source": [
    "The principal components are those uncorrelated linear combinations $y_1, \\cdots, y_p$ whose variances are as large as possible.\n",
    " \n",
    "Remark: $\\mathbf{y}$ is often referred to as the transformed data matrix of $\\mathbf x$, where $\\mathbf x$ is projected onto the new set of principal components (eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e492c-6323-4aaa-8c6d-79e5a7e94722",
   "metadata": {},
   "source": [
    "### 1.2 Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063de81-fd68-4914-8233-4ac21ca449e0",
   "metadata": {},
   "source": [
    "Let the random vector $\\mathbf x = (x_1, \\cdots, x_p)^\\top$ have the covariance matrix $\\mathbf \\Sigma$ with eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq 0$. \n",
    "\n",
    "Consider the linear combinations\n",
    "\\begin{equation*}\n",
    "\t\\mathbf y := \\left(\\begin{array}{c}\n",
    "\t\ty_1\t\\\\\t\\vdots\t\\\\\ty_p\n",
    "\t\\end{array}\\right) = \\left(\\begin{array}{c}\n",
    "\t\t\\mathbf a_1^\\top \\mathbf x \\\\ \\vdots \\\\ \\mathbf a_p^\\top \\mathbf x\n",
    "\t\\end{array}\\right) = \\mathbf A \\mathbf x \\, .\n",
    "\\end{equation*}\n",
    "Then, for $i,j=1,2,\\cdots,p$, we have\n",
    "\\begin{align*}\n",
    "\t{\\rm Var}(y_i) &= {\\rm Var}(\\mathbf a_i^\\top \\mathbf x) = \\mathbf a_i^\\top \\mathbf \\Sigma \\mathbf a_i \\, , \\\\\n",
    "\t{\\rm Cov}(y_i, y_j) &= {\\rm Cov}(\\mathbf a_i^\\top \\mathbf x, \\mathbf a_j^\\top \\mathbf x) = \\mathbf a_i^\\top \\mathbf \\Sigma \\mathbf a_j \\, .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f28a35-214e-43fd-a5d6-d4472b2101a4",
   "metadata": {},
   "source": [
    "<u>Thinking</u>\n",
    "\n",
    "But why do we need to maximize the variance and what can this achieve? Why maximized variance helps preserve the information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a632a-4d37-4fc9-b8b7-3eacf93f5489",
   "metadata": {},
   "source": [
    "### 1.3 Properties of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea49a4-a8f2-4810-8956-a4bb9947c861",
   "metadata": {},
   "source": [
    "Let $\\mathbf \\Sigma$ be the covariance matrix associated with the random vector $\\mathbf x^\\top = (x_1, \\cdots, x_p)^\\top$. \n",
    "\n",
    "Let $\\mathbf \\Sigma$ have the normalized eigenvalue-eigenvector pairs $(\\lambda_1, \\mathbf e_1 ), \\cdots, (\\lambda_p, \\mathbf e_p)$, where $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_p \\geq 0$. \n",
    "\n",
    "Then the $i$-th principal component is given by\n",
    "\\begin{equation*}\n",
    "\ty_i = \\mathbf e_i^\\top \\mathbf x = e_{i1} x_1 + e_{i2} x_2 + \\cdots + e_{ip} x_p \\, , \\qquad i=1,2, \\cdots, p \\, ,\n",
    "\\end{equation*}\n",
    "with\n",
    "\\begin{alignat*}{2}\n",
    "\t{\\rm Var}(y_i) &= \\mathbf e_i^\\top \\mathbf \\Sigma \\mathbf e_i = \\lambda_i \\, , \\qquad &&i=1,2, \\cdots, p \\, , \\\\\n",
    "\t{\\rm Cov}(y_i, y_j) &= \\mathbf e_i^\\top \\mathbf \\Sigma \\mathbf e_j = 0 \\, , \\qquad &&i \\neq j \\, .\n",
    "\\end{alignat*}\n",
    "If some $\\lambda_i$'s are equal, the choices of the corresponding coefficient vectors, $\\mathbf e_i$, and hence $y_i$, are not unique.\n",
    "\n",
    "<u>Remark:</u>\n",
    "\n",
    "$\\mathbf e_i^\\top\\mathbf e_j = 0,\\, \\text{for $i\\neq j$}$\n",
    "\n",
    "Why is this true? What is the relationship between it and $Cov(y_i, y_j)=0$? Try to argue and prove yourself.\n",
    "\n",
    "\\\n",
    "The above process is actually <b>spectral decomposition</b>:\n",
    "\n",
    " \n",
    "Given a square and symmetric matrix A, then it can be decomposed into $A = QDQ^{\\top}$, \n",
    "\n",
    "$Q$ is the matrix where each column is an eigenvector of $A$ ($Q$ is an orthogonal matrix since $A$ is symmetric), \n",
    "\n",
    "$D$ is a diagonal matrix where the diagonal entries are the eigenvalues of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd32440-7ba8-4667-b64a-055743c6bd79",
   "metadata": {},
   "source": [
    "### 1.4 Fitting Data with SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d3c89-c8c0-43e9-abe0-740778a67820",
   "metadata": {},
   "source": [
    "Given demeaned data $\\mathbf{X_{T \\times p}}$, via PCA, we can $r$-rank approximate as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{\\tilde{X}_{T \\times p}} = \\mathbf{U}_{T \\times r} \\mathbf{\\Sigma}_{r \\times r} \\mathbf{V}^\\top_{r \\times p} = \\sum_{k=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\n",
    "\\end{equation*}\n",
    "\n",
    "$\\mathbf{\\Sigma}$ contains the <b>singular values</b> of $\\mathbf X$, which is the same as the square root of eigenvalues of $\\mathbf {X ^\\top X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8befa-6f4c-4517-ad75-e302bd399c62",
   "metadata": {},
   "source": [
    "## 2. Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e5532-6bfa-4c30-8e2e-144f929f5dcf",
   "metadata": {},
   "source": [
    "### 2.1 Approximation of Portfolio V@R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b37b2-296f-4357-b551-08bfae78463b",
   "metadata": {},
   "source": [
    "We need the standard deviation of the loss in a portfolio to compute the $V@R$ of that portfolio. \n",
    "\n",
    "Let the change in the portfolio value be $\\Delta P=w^\\top x=w_1x_1+ \\ldots + w_p x_p$ \n",
    "\n",
    "and we can approximate $\\Delta P$ by using the first $m$ PCs of $x$ as follows (with $p >> m$):\n",
    "\n",
    "\\begin{align*} \n",
    "\t\\Delta P  &= w^\\top x \\approx w_1(h_{11}y_1+ \\ldots +h_{1m}y_m + \\epsilon_1)+ \\ldots + w_p(h_{p1}y_1+ \\ldots +h_{pm}y_m + \\epsilon_p) &\\\\\n",
    "\t&= (w_1h_{11}+ \\ldots + w_p h_{p1})y_1 +\\ldots + (w_1 h_{1m} + \\ldots +w_ph_{pm})y_m + (w_1 \\epsilon_{1} + \\ldots + w_p \\epsilon_{p})\\\\\n",
    "\t&=: \\delta_1 y_1+\\ldots + \\delta_m y_m + \\epsilon, &\n",
    "\\end{align*}\n",
    "\n",
    "where $\\delta_k = \\sum_{i=1}^p w_i h_{ik}$, for $k=1,2,\\ldots,m$ and $\\epsilon = \\sum_{i=1}^p w_i \\epsilon_i$ (idiosyncratic risk not captured by factors). \n",
    "\n",
    "The $N$-day 99\\% $V@R$ of $\\Delta P$ is $z_{0.99}\\sqrt{N Var (\\Delta P)}$, we can just use the first $m$ PCs to approximate the variance of $\\Delta P$,\n",
    "\n",
    "$$Var(\\Delta P) \\approx \\delta_1^2 Var(y_1)+ \\cdots + \\delta_m^2 Var(y_m)=\\delta_1^2 \\lambda_1+ \\cdots +\\delta_m^2 \\lambda_m.$$\n",
    "\n",
    "This expression is much simpler due to the fact that all PCs are orthogonal and maintain a large portion of the variance contained in $x$.\n",
    "\n",
    "\\\n",
    "<u>Note</u> \n",
    "\n",
    "If the unexplained variance $\\epsilon$ is in fact large (let's say $\\ge 10 \\%$ of total variance), \n",
    "\n",
    "then we can use the following fact:\n",
    "\n",
    "\\begin{equation*} \n",
    "    Var(\\epsilon_i) = Var(x_i) - \\sum_{k=1}^m h_{ik}^2 \\lambda_k , \\qquad i=1,2, \\cdots, p\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "By assuming independence of $\\epsilon_i$ and $y_k$:\n",
    "\n",
    "\\begin{equation*} \n",
    "    Var(\\Delta P) \\approx \\sum_{k=1}^m \\delta_k^2 \\lambda_k + \\sum_{i=1}^p w_i^2 Var(\\epsilon_i)\n",
    "\\end{equation*}\n",
    "\n",
    "Essentially, we have broken down risk into systematic ($\\delta_k$ means factor exposure) and idiosyncratic components. \n",
    "\n",
    "Advantage is that all the cross-correlations are modelled through the factor commonalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62fd7b-55b3-4fbf-8e38-9c69c0621e94",
   "metadata": {},
   "source": [
    "### 2.2 Implementation: Stock Trends and Investment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeab6b6-db91-4c8d-ab15-850e6d6d0c32",
   "metadata": {},
   "source": [
    "In fact, in finance, one may also employ PCA for dimensionality reduction and find stocks with favorable trends to invest in. \n",
    "\n",
    "Specifically, we might figure out the current market trend as PCA decomposes the market views into independent segments.\n",
    "\n",
    "Hopefully, we can follow the momentum and build a winning portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4ea00-684f-4c98-844a-e13478f24f54",
   "metadata": {},
   "source": [
    "#### 2.2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa8e70-9641-42c0-bdd0-c20fa8e980d0",
   "metadata": {},
   "source": [
    "Below we have the normalized returns of the S&P 500 components from 2020-01-01 to 2020-06-30.\n",
    "\n",
    "The normalization is done as dividing the raw returns by its rolling 65-day volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75852ef-bf1f-42f8-b249-4e10cce482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_norm = pd.read_parquet('rets_norm_covid.parquet')\n",
    "rets_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2e510-b830-493b-ac85-ef9d1492004a",
   "metadata": {},
   "source": [
    "#### 2.2.2 PCA Eigenvalue and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef8643-1d38-4ffe-9a31-6a7add83f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "fit_data = rets_norm.clip(-5, 5).fillna(0)\n",
    "\n",
    "pca_mod = PCA(n_components=n_components, random_state=0)\n",
    "pca_mod.fit(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2291974-ca66-4d5f-a5a2-e987f69d1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vec = pd.DataFrame(pca_mod.components_, columns=fit_data.columns, index=range(1, n_components + 1))\n",
    "eig_vec = eig_vec.rename_axis(index='PC').rename_axis(columns='sec')\n",
    "eig_vec = eig_vec.mul(np.sign(eig_vec.mean(axis=1)), axis=0) # Force positive 'orientation'\n",
    "eig_val = pd.Series(pca_mod.explained_variance_ratio_, index=range(1, n_components + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38f9c6-b3ec-4862-ab89-cc87f8a654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vec.T.hist(layout=(4, 3), figsize=(15, 8), bins=50);\n",
    "plt.suptitle(f'Histograms of Top {n_components} PC Loadings');\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ab032-85d2-4cd9-ace3-782b2da6e930",
   "metadata": {},
   "source": [
    "It is well known that PC1 is the \"market\", which means to long all stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37d1ee-76b9-4372-b7f7-41109f2f6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(eig_val * 100).plot.barh(title=f'Explained Variance Ratio by top {n_components} PCs')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b39ce-8a74-4e96-9285-156e950f744b",
   "metadata": {},
   "source": [
    "Market has $> 60 \\%$ variance while PC2 is already $<10 \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d930ed5-8fbe-4b04-9056-e3b97e73dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = 252\n",
    "factor_ret = fit_data @ eig_vec.T\n",
    "factor_sr = factor_ret.mean() / factor_ret.std() * np.sqrt(ann)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10));\n",
    "\n",
    "factor_ret.cumsum().plot(ax=ax[0], title=f'PnL of top {n_components} factor returns');\n",
    "factor_sr.abs().plot.bar(title=f'Absolute Sharpe Ratio of top {n_components} factor returns', ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7ade3-b732-4104-a111-7abe6389c683",
   "metadata": {},
   "source": [
    "Remark: I plotted out the absolute Sharpe Ratio because PCA is sign invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f2a9a-8120-4c98-b394-8f2e35cc2225",
   "metadata": {},
   "source": [
    "#### 2.2.3 Sector Analysis on PCA factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8a17c-e582-4e47-9462-02fe6f9ab752",
   "metadata": {},
   "source": [
    "We can classify stocks into different sectors according to GICS.\n",
    "\n",
    "Let's analyze the most interesting two PCs: PC2 and PC4 as they have a high absolute Sharpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d3db2-8d85-417a-8942-f27ab8799048",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = 2\n",
    "top_bottom_n = 5\n",
    "title=f\"Loadings of Top and bottom {top_bottom_n} stocks in PC{pc}\"\n",
    "pc_stocks = eig_vec.loc[pc].sort_values().iloc[np.r_[0:top_bottom_n, -top_bottom_n:0]]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10));\n",
    "\n",
    "pc_stocks.plot.bar(title=title, ax=ax[1]);\n",
    "ax[1].set_xticklabels(labels=pc_stocks.index, rotation=30);\n",
    "\n",
    "rets_norm.loc[:, pc_stocks.index].cumsum().plot(ax=ax[0], title=f\"PnL of Top and bottom {top_bottom_n} stocks in PC{pc}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6b24f-f47c-4370-be2a-c1e6ea387e35",
   "metadata": {},
   "source": [
    "<u>Long</u>\n",
    "\n",
    "Energy: WEC, ES, ED, CMS\n",
    "\n",
    "Utilities: AWK\n",
    "\n",
    "<u>Short</u>\n",
    "\n",
    "Consumer Discretionary: \n",
    "\n",
    "1. Cruise: NCLH, RCL, CCL\n",
    "2. Hotel: MAR\n",
    "3. Airline: UAL\n",
    "\n",
    "-> Undoubtedly, the travel industry is much impacted. \n",
    "\n",
    "While on the long side, it is well known that many traders speculate on the spread between energy and cruise & airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7c12b-d51c-45a6-b07c-1e415fcf57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = 4\n",
    "top_bottom_n = 5\n",
    "title=f\"Loadings of Top and bottom {top_bottom_n} stocks in PC{pc}\"\n",
    "pc_stocks = eig_vec.loc[pc].sort_values().iloc[np.r_[0:top_bottom_n, -top_bottom_n:0]]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10));\n",
    "\n",
    "pc_stocks.plot.bar(title=title, ax=ax[1]);\n",
    "ax[1].set_xticklabels(labels=pc_stocks.index, rotation=30);\n",
    "\n",
    "rets_norm.loc[:, pc_stocks.index].cumsum().plot(ax=ax[0], title=f\"PnL of Top and bottom {top_bottom_n} stocks in PC{pc}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4dc478-cd00-4c8b-bb66-4fd57d61fd60",
   "metadata": {},
   "source": [
    "Similarly, we see packaging sectors are affected, maybe because the transportation of goods are suspended.\n",
    "\n",
    "Meanwhile, IT firms like MSFT benefit from work from home policy. \n",
    "\n",
    "Chipotle also gained because of food delivery demands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
